---
title: "Business Decision Algorithims"
author: 'By: Dominique Miranda'
output: html_document
---

>Case: A movie chain in the southwest region, MovieMagic is considering ways in which it can
increase spending on concessions. It has collected information of 2000 of its customers,
some of whom are part of their loyalty program and some who are not. They have
information on the following 8 variables, which they plan to use as predictors. They plan to use amount_spent (i.e. the amount spent on concessions) as the outcome variable
since they have learnt from observations that much of the profit is derived from concession
sales.

```{r}
setwd("~/MKTG-6600")

# install libraries
library(dplyr)
library(tidyverse)
library(topicmodels)
library(ggplot2)
library(tidytext)
library(tidyr)
library(RTextTools)
library(wordcloud)
library(tm)
library(quanteda)
library(reshape2)
library(quanteda.textplots)
library(car)
library(glmnet)
library(caret)
library(ROCR)
library(psycho)
library(textdata)
library(correlation)
library(reticulate)
```

**#1. Regression analysis**
```{r, warning=FALSE, message=FALSE}
# Read in the data
data1 <- read.csv("http://data.mishra.us/files/project_data.csv")

#linear regression
model1<- lm(amount_spent~., data=data1)
summary(model1) #each level of each categorical predictor

```
>Job and education are significant predictors due to p-value greater than 0.05.

**#2.Penalized Regression**
```{r, warning=FALSE, message=FALSE}
vif(model1) #test for multicollinearity

data2<- read.csv(url("http://data.mishra.us/files/project_data.csv"))

#Selecting relevant predictors using penalized regression
correlation <- correlation(data2)
ggcorrplot::ggcorrplot(correlation,hc.order=TRUE, type="lower",lab=TRUE)

x <- model.matrix(amount_spent~., data2)[,-1]
y <- data1$amount_spent

#LASSO
lasso_model <- cv.glmnet(x, y, alpha = 1)
plot(lasso_model)

#Ridge
ridge_model <- cv.glmnet(x, y, alpha = 0)
plot(ridge_model)
```

>Since values of greater than 1.5 indicate multicolinearity, job and education are multicolinear. The most relevant predictors for understanding their positive influence on the outcome variable are amount spent & days member & movies seen, according to the correlation matrix diagram. In regards to negative influence, age & streaming hold that influence.

**#4 & 5 Predictive model**
```{r, warning=FALSE, message=FALSE}
#training the model 70% & 30%
set.seed(1234)
datasplit <- createDataPartition(data1$amount_spent, p = 0.7, list=FALSE)
trainData <- data1[datasplit,]
testData <- data1[-datasplit,]

#obtaining predictions from the model
lmregression<- train(amount_spent~., data=trainData, method = "lm",
na.action=na.exclude)
summary(lmregression)

#calculating RMSE, R-squared, and MAE for the predicted model values
predictions <-predict(lmregression, newdata=testData)
postResample(predictions, testData$amount_spent)
```
>The R-squared for the linear regression model with 70% & 30% set is 0.554890. The RMSE is 9.788593 and the MAE is 7.802252

```{r}
#training the model 80% & 20%
set.seed(1234)
datasplit <- createDataPartition(data1$amount_spent, p = 0.8, list=FALSE)
trainData <- data1[datasplit,]
testData <- data1[-datasplit,]

#obtaining predictions from the model
lmregression<- train(amount_spent~., data=trainData, method = "lm",
na.action=na.exclude)
summary(lmregression)

#calculating RMSE, R-squared, and MAE for the predicted model values
predictions <-predict(lmregression, newdata=testData)
postResample(predictions, testData$amount_spent)
```

>The R-squared for the linear regression model with 80% & 20% set is 0.6674792. The RMSE is 9.6496312 and the MAE is 7.6528047.

>Analysis 1-5:

> 1. The predictors that had significant influence on amount spent on concessions are job and education. If a customer has an education, they are more likely to have a job. And in turn, have a steady income to buy food from concession stands at the movies. Since values of greater than 1.5 indicate multicolinearity, job and education are multicolinear.

> 2. The predictors that have a positive influence are amount spent & days member at 0.79 correlation & movies seen & amount spent at 0.22. The predictors that have negative influence are age & streaming with a correlation of -0.03. The penalized regression analysis helped answer this question through the use of the correlation matrix. Yes, a neural net model can help find the significant (or not), predictors, magnitude, and direction of influence because knowing which customers are likely to drop-off versus those that have stayed allows a firm to come up with customized strategies for disappointed customer's reviews to be addressed.

> 3. Penalized regression helped me select the relevant variables of by summarizing each level of each categorical predictor correlation, while addressing multicolinearity. Predictor variables I would use in the model are job and education because these variables are the ones with a significant p-value. A LASSO would help in selecting relevant predictors through the use of the VIF model because it can shrink the estimates to zero for less relevant predictors. MovieMagic would then knows which predictors are relevant to them and can focus their resources and improve sales.

> 4.The R-squared for the linear regression model with 70% & 30% set is 0.554890. The RMSE is 9.788593 and the MAE is 7.802252. The R-squared for the linear regression model with 80% & 20% set is 0.6674792. The RMSE is 9.6496312 and the MAE is 7.6528047. As a result, when tge data us sokue 80-20, the R-squared value increases, the RMSE value decreases, and the MAE value decreases.

> 5. Since job and education are significant predictors, some strategies that MovieMagic can come up with to increase amount spent on concessions is to market to offices of employment. MovieMagic can hand out flyer coupons indicating a certain spending rate will get the customer a free treat or a certain percent off their purchase. This will drive the items per order to increase concession sales.

**#6.Text Analysis**
```{r, warning=FALSE, message=FALSE}
text <- read.csv("http://data.mishra.us/files/project_reviews.csv")

#positive ratings
positive <- text %>%
  filter((star >= 3)) 

positive$text <- as.character(positive$text)
set.seed(1234)
pos_corpus <- VCorpus(VectorSource(positive$text))

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
pos_corpus <- tm_map(pos_corpus, toSpace, "/|@|\\|")
pos_corpus <- tm_map(pos_corpus, stripWhitespace) # remove white space
pos_corpus <- tm_map(pos_corpus, content_transformer(tolower))
pos_corpus <- tm_map(pos_corpus, removeNumbers) # remove numbers
pos_corpus <- tm_map(pos_corpus, removePunctuation) # remove punctuations
pos_corpus <- tm_map(pos_corpus, removeWords, stopwords("en"))

#Term Document Matrix
dtm <- TermDocumentMatrix(pos_corpus,control = list(weighting = weightTfIdf))
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

#wordcloud based on POSITIVE valence
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1, max.words=200, random.order=FALSE,
          rot.per=0.35,colors=brewer.pal(8, "Dark2"),scale=c(3, 0.7))


#negative ratings
negative <- text %>%
  filter((star < 3)) 

negative$text <- as.character(negative$text)
set.seed(1234)
neg_corpus <- VCorpus(VectorSource(negative$text))

toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
neg_corpus <- tm_map(neg_corpus, toSpace, "/|@|\\|")
neg_corpus <- tm_map(neg_corpus, stripWhitespace) # remove white space
neg_corpus <- tm_map(neg_corpus, content_transformer(tolower))
neg_corpus <- tm_map(neg_corpus, removeNumbers) # remove numbers
neg_corpus <- tm_map(neg_corpus, removePunctuation) # remove punctuations
neg_corpus <- tm_map(neg_corpus, removeWords, stopwords("en"))

#Term Document Matrix
neg_dtm <- TermDocumentMatrix(neg_corpus,control = list(weighting = weightTfIdf))
neg_m <- as.matrix(neg_dtm)
neg_v <- sort(rowSums(neg_m),decreasing=TRUE)
neg_d <- data.frame(word = names(neg_v),freq=neg_v)

#wordcloud based on NEGATIVE valence
set.seed(1234)
wordcloud(words = neg_d$word, freq = neg_d$freq, min.freq = 1, max.words=100, random.order=FALSE,
          rot.per=0.35,colors=brewer.pal(8, "Dark2"),scale=c(3, 0.7))
```

>First, 'great' appears as one of the most frequently used words. Second, experience is as frequently mentioned as 'service', so customers expect a top tier atmosphere along with excellent customer service. This could then lead to how well a concession item is serviced to a customer that good lead to a postive or negative review, such as whether or not their popcorn was hot and fresh leaving a 'bad' review from the negative wordcloud.

**#7.Topic Model**
```{r}
#Latent Dirichlet Analysis
text <- read.csv("http://data.mishra.us/files/project_reviews.csv")

#remove stop words
corpus <- VCorpus(VectorSource(text$text))

#function to clean /,@,\\,|
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "/|@|\\|")
corpus<- tm_map(corpus, stripWhitespace) # remove white space

#convert all to lower case else same word as lower and uppercase will classified as different
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers) # remove numbers
corpus <- tm_map(corpus, removePunctuation) # remove punctuations
corpus <- tm_map(corpus, removeWords, stopwords("en"))
dtm <- DocumentTermMatrix(corpus)

set.seed(234)
rowTotals <- apply(dtm , 1, sum)
dtm <- dtm[rowTotals> 0, ]
lda <- LDA(dtm, k = 3, method = "Gibbs", control = NULL)
topics <- tidy(lda, matrix = "beta") # beta is the topic-word density

top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>% # top_n picks 10 topics.
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

>Analysis 6-8:

> 6. The prominent words in the positive ratings are 'great', 'movie', and 'love'. The prominent words in the negative ratings are 'order','lasagna', 'meal', and 'theater'. Some strategies that can be developed in messaging customers could be to offer a free food voucher for a bad review to increase customer satisfaction, as some bad reviews were centered around the food service.Another strategy could be that customers are cold when they go to the movies and MovieMagic should keep the temperatures lower. This integrates with the regression insights because with the regression analysis we developed a theory to hand out flyers to people with jobs & an education in industrial districts--where we are most likely to find this demographic. 

> 7. The term that is the most relevant in topic 1 is great, in topic 2 is movie, and in topic 3 is food. This would inform my business strategy by notifying MovieMagic that if MovieMagic wants to increase concession sales, I recommend them focusing on the three topics so that they can provide superior experience in those topics. This will lead to increase in sales as customers are noticing these topics the most according the the review. This leads to MovieMagic either upping food orders or for an example, popcorn quality. Yes, I would recommend promotions such as a 'buy 3, get one free' to get coustomers to buy more food and in turn, increase sales. A loyaly program would also be a good idea because it establishes a line from the business to the customer and the customer will be more than likely to come back again for these points.

> 8.The experiment that I would recommend is an ANOVA because it compares the mean values of each of the experimental conditions to test for the null hypothesis for the best fit. The benefits of ANOVA is its ability to compare means across three or more groups. The experimental design would be a 3 cell design that uses the top 3 terms influence to concession sales. This would lead to a deeper understanding of what business strategies would work because it would bring a better understanding between the predictors or factors on the outcome and each of the factors has multiple levels.
